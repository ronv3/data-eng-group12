services:
  airflow-db:
    image: postgres:16.4
    container_name: airflow-db-tourism
    environment:
      POSTGRES_USER: ${POSTGRES_USER:-airflow}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-airflow}
      POSTGRES_DB: ${POSTGRES_DB:-airflow}
      POSTGRES_INITDB_ARGS: "--auth=md5"
    healthcheck:
      test: [ "CMD-SHELL", "pg_isready -h localhost -U postgres -d postgres" ]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 30s
    volumes:
      - airflow_db_data:/var/lib/postgresql/data

  clickhouse:
    image: clickhouse/clickhouse-server:24.3
    container_name: clickhouse
    environment:
      - CLICKHOUSE_DB=default
      - CLICKHOUSE_USER=${CLICKHOUSE_USER:-default}
      - CLICKHOUSE_PASSWORD=${CLICKHOUSE_PASSWORD:-}
      - TZ=${TZ:-UTC}
      - CLICKHOUSE_DEFAULT_ACCESS_MANAGEMENT=1
    ports:
      - "8123:8123"     # HTTP interface
    volumes:
      - clickhouse_data:/var/lib/clickhouse
      - iceberg_warehouse:/warehouse/iceberg
    healthcheck:
      test: [ "CMD-SHELL", "clickhouse-client --host 127.0.0.1 --query 'SELECT 1' >/dev/null 2>&1" ]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 20s

  # -------- Iceberg stack --------
  minio:
    image: minio/minio:RELEASE.2024-10-13T13-34-11Z
    container_name: minio
    command: server /data --console-address ":9001"
    ports:
      - "9000:9000"   # MinIO API (S3)
      - "9001:9001"   # MinIO Console
    environment:
      MINIO_ROOT_USER: ${S3_ACCESS_KEY_ID:-minioadmin}
      MINIO_ROOT_PASSWORD: ${S3_SECRET_ACCESS_KEY:-minioadmin}
    volumes:
      - minio_data:/data
    restart: unless-stopped

  minio-mc:
    image: minio/mc
    container_name: minio-mc
    depends_on:
      - minio
    entrypoint: ["/bin/sh","-lc"]
    command: >
      '
      mc alias set local http://minio:9000 ${S3_ACCESS_KEY_ID:-minioadmin} ${S3_SECRET_ACCESS_KEY:-minioadmin} --api s3v4 &&
      mc mb --ignore-existing local/${S3_BUCKET:-tourism-warehouse} &&
      echo "Bucket ready"
      '
    restart: "no"

  iceberg_rest:
    image: tabulario/iceberg-rest:1.6.0
    container_name: iceberg_rest
    ports:
      - "8181:8181"
    environment:
      AWS_ACCESS_KEY_ID: ${S3_ACCESS_KEY_ID:-minioadmin}
      AWS_SECRET_ACCESS_KEY: ${S3_SECRET_ACCESS_KEY:-minioadmin}
      AWS_REGION: ${S3_REGION:-us-east-1}
      AWS_ENDPOINT: http://minio:9000
      AWS_S3_FORCE_PATH_STYLE: "true"
      CATALOG_WAREHOUSE: s3://${S3_BUCKET:-tourism-warehouse}/
      CATALOG_IO__IMPL: org.apache.iceberg.aws.s3.S3FileIO
      CATALOG_S3_ENDPOINT: http://minio:9000
      CATALOG_S3_PATH-STYLE-ACCESS: "true"
    depends_on:
      - minio
      - minio-mc
    restart: unless-stopped

  airflow-init:
    build:
      context: .
      dockerfile: docker/Dockerfile.airflow
    image: tourism-airflow:2.8.1
    container_name: airflow-init-tourism
    entrypoint: /bin/bash
    command:
      - -lc
      - |
        set -euo pipefail
        airflow db migrate
        airflow users create \
          --username "${AIRFLOW_USER:-admin}" \
          --password "${AIRFLOW_PASSWORD:-admin}" \
          --firstname "${AIRFLOW_FIRSTNAME:-Admin}" \
          --lastname "${AIRFLOW_LASTNAME:-User}" \
          --role Admin \
          --email "${AIRFLOW_EMAIL:-admin@example.com}" || true
    environment:
      PYTHONPATH: /opt/airflow
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER:-airflow}:${POSTGRES_PASSWORD:-airflow}@airflow-db:5432/${POSTGRES_DB:-airflow}
      AIRFLOW__CORE__DAGS_FOLDER: /dev/null
      PYICEBERG_CATALOG__REST__S3__PATH_STYLE_ACCESS: "true"
      # PyIceberg env
      ICEBERG_CATALOG_URI: ${ICEBERG_CATALOG_URI:-http://iceberg_rest:8181/}
      PYICEBERG_CATALOG__REST__URI: ${ICEBERG_CATALOG_URI:-http://iceberg_rest:8181/}
      PYICEBERG_CATALOG__REST__WAREHOUSE: s3://${S3_BUCKET:-tourism-warehouse}/
      PYICEBERG_CATALOG__REST__IO__IMPL: org.apache.iceberg.aws.s3.S3FileIO
      PYICEBERG_CATALOG__REST__S3__ENDPOINT: http://minio:9000
      PYICEBERG_CATALOG__REST__S3__PATH-STYLE-ACCESS: "true"
      PYICEBERG_CATALOG__REST__S3__ACCESS-KEY-ID: ${S3_ACCESS_KEY_ID:-minioadmin}
      PYICEBERG_CATALOG__REST__S3__SECRET-ACCESS-KEY: ${S3_SECRET_ACCESS_KEY:-minioadmin}
      PYICEBERG_CATALOG__REST__S3__REGION: ${S3_REGION:-us-east-1}
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/include:/opt/airflow/include
      - ./airflow/plugins:/opt/airflow/plugins
      - ./src:/opt/airflow/src
      - ./dbt:/opt/airflow/dbt
      - iceberg_warehouse:/warehouse/iceberg
    depends_on:
      airflow-db:
        condition: service_healthy
    restart: "no"

  airflow-scheduler:
    build:
      context: .
      dockerfile: docker/Dockerfile.airflow
    image: tourism-airflow:2.8.1
    container_name: airflow-scheduler-tourism
    command: scheduler
    environment:
      PYTHONPATH: /opt/airflow
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
      AIRFLOW__API__AUTH_BACKENDS: "airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session"
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER:-airflow}:${POSTGRES_PASSWORD:-airflow}@airflow-db:5432/${POSTGRES_DB:-airflow}
      CLICKHOUSE_HOST: clickhouse
      CLICKHOUSE_PORT: 8123
      CLICKHOUSE_USER: ${CLICKHOUSE_USER:-default}
      CLICKHOUSE_PASSWORD: ${CLICKHOUSE_PASSWORD:-}
      CLICKHOUSE_DB_BRONZE: bronze
      CLICKHOUSE_DB_SILVER: silver
      CLICKHOUSE_DB_GOLD: gold
      DBT_PROFILES_DIR: /opt/airflow/dbt
      PYICEBERG_CATALOG__REST__S3__PATH_STYLE_ACCESS: "true"
      # PyIceberg env
      ICEBERG_CATALOG_URI: ${ICEBERG_CATALOG_URI:-http://iceberg_rest:8181/}
      PYICEBERG_CATALOG__REST__URI: ${ICEBERG_CATALOG_URI:-http://iceberg_rest:8181/}
      PYICEBERG_CATALOG__REST__WAREHOUSE: s3://${S3_BUCKET:-tourism-warehouse}/
      PYICEBERG_CATALOG__REST__IO__IMPL: org.apache.iceberg.aws.s3.S3FileIO
      PYICEBERG_CATALOG__REST__S3__ENDPOINT: http://minio:9000
      PYICEBERG_CATALOG__REST__S3__ACCESS-KEY-ID: ${S3_ACCESS_KEY_ID:-minioadmin}
      PYICEBERG_CATALOG__REST__S3__SECRET-ACCESS-KEY: ${S3_SECRET_ACCESS_KEY:-minioadmin}
      PYICEBERG_CATALOG__REST__S3__REGION: ${S3_REGION:-us-east-1}
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/include:/opt/airflow/include
      - ./airflow/plugins:/opt/airflow/plugins
      - ./src:/opt/airflow/src
      - ./dbt:/opt/airflow/dbt
      - ./logs:/opt/airflow/logs
    depends_on:
      airflow-db:
        condition: service_healthy
      airflow-init:
        condition: service_completed_successfully
      iceberg_rest:
        condition: service_started
      minio-mc:
        condition: service_started
    restart: always

  airflow-webserver:
    build:
      context: .
      dockerfile: docker/Dockerfile.airflow
    image: tourism-airflow:2.8.1
    container_name: airflow-webserver-tourism
    command: webserver
    ports:
      - "8080:8080"
    environment:
      PYTHONPATH: /opt/airflow
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
      AIRFLOW__API__AUTH_BACKENDS: "airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session"
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER:-airflow}:${POSTGRES_PASSWORD:-airflow}@airflow-db:5432/${POSTGRES_DB:-airflow}
      CLICKHOUSE_HOST: clickhouse
      CLICKHOUSE_PORT: 8123
      CLICKHOUSE_USER: ${CLICKHOUSE_USER:-default}
      CLICKHOUSE_PASSWORD: ${CLICKHOUSE_PASSWORD:-}
      CLICKHOUSE_DB_BRONZE: bronze
      CLICKHOUSE_DB_SILVER: silver
      CLICKHOUSE_DB_GOLD: gold
      DBT_PROFILES_DIR: /opt/airflow/dbt
      PYICEBERG_CATALOG__REST__S3__PATH_STYLE_ACCESS: "true"
      # PyIceberg env
      ICEBERG_CATALOG_URI: ${ICEBERG_CATALOG_URI:-http://iceberg_rest:8181/}
      PYICEBERG_CATALOG__REST__URI: ${ICEBERG_CATALOG_URI:-http://iceberg_rest:8181/}
      PYICEBERG_CATALOG__REST__WAREHOUSE: s3://${S3_BUCKET:-tourism-warehouse}/
      PYICEBERG_CATALOG__REST__IO__IMPL: org.apache.iceberg.aws.s3.S3FileIO
      PYICEBERG_CATALOG__REST__S3__ENDPOINT: http://minio:9000
      PYICEBERG_CATALOG__REST__S3__ACCESS-KEY-ID: ${S3_ACCESS_KEY_ID:-minioadmin}
      PYICEBERG_CATALOG__REST__S3__SECRET-ACCESS-KEY: ${S3_SECRET_ACCESS_KEY:-minioadmin}
      PYICEBERG_CATALOG__REST__S3__REGION: ${S3_REGION:-us-east-1}
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/include:/opt/airflow/include
      - ./airflow/plugins:/opt/airflow/plugins
      - ./src:/opt/airflow/src
      - ./dbt:/opt/airflow/dbt
      - ./logs:/opt/airflow/logs
    depends_on:
      airflow-db:
        condition: service_healthy
      airflow-init:
        condition: service_completed_successfully
      iceberg_rest:
        condition: service_started
      minio-mc:
        condition: service_started
    restart: always

    # -------------------- OpenMetadata stack (profile: omd) --------------------
  openmetadata-mysql:
    profiles: [ "omd" ]
    image: docker.getcollate.io/openmetadata/db:1.10.9   # match your server:1.10.9
    container_name: openmetadata-mysql
    environment:
      MYSQL_ROOT_PASSWORD: ${OM_MYSQL_ROOT_PASSWORD:-om_root}
      MYSQL_USER: ${OM_DB_USER:-openmetadata}
      MYSQL_PASSWORD: ${OM_DB_PASSWORD:-openmetadata}
      MYSQL_DATABASE: ${OM_DB_NAME:-openmetadata_db}
    volumes:
      - om_mysql_data:/var/lib/mysql
    healthcheck:
      test: [ "CMD-SHELL", "mysqladmin ping -u root -p$${MYSQL_ROOT_PASSWORD} --silent" ]
      interval: 10s
      timeout: 5s
      retries: 20
      start_period: 30s
    restart: unless-stopped

  openmetadata-elasticsearch:
    profiles: ["omd"]
    image: docker.elastic.co/elasticsearch/elasticsearch:8.11.4
    container_name: openmetadata-elasticsearch
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
      - ES_JAVA_OPTS=-Xms1g -Xmx1g
      - ingest.geoip.downloader.enabled=false
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - om_es_data:/usr/share/elasticsearch/data
    healthcheck:
      test: ["CMD-SHELL", "curl -s http://localhost:9200 >/dev/null || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 30
      start_period: 30s
    ports:
      - "9200:9200"   # optional for local debugging
    restart: unless-stopped

  # One-shot DB migration for OpenMetadata
  openmetadata-migrations:
    profiles: [ "omd" ]
    image: openmetadata/server:1.10.9
    container_name: openmetadata-migrations
    depends_on:
      openmetadata-mysql:
        condition: service_healthy
    environment:
      DB_HOST: openmetadata-mysql
      DB_PORT: 3306
      DB_SCHEME: mysql
      DB_DRIVER_CLASS: com.mysql.cj.jdbc.Driver
      DB_PARAMS: allowPublicKeyRetrieval=true&useSSL=false&serverTimezone=UTC
      DB_USER: ${OM_DB_USER:-openmetadata}
      DB_USER_PASSWORD: ${OM_DB_PASSWORD:-openmetadata}
      OM_DATABASE: ${OM_DB_NAME:-openmetadata_db}
    volumes:
      - ./openmetadata/config/openmetadata.yaml:/openmetadata/conf/openmetadata.yaml:ro
    entrypoint: [ "/bin/bash", "-lc" ]
    command: >
      ./bootstrap/openmetadata-ops.sh migrate
      --openmetadata-config /openmetadata/conf/openmetadata.yaml
    restart: "no"



  openmetadata-server:
    profiles: [ "omd" ]
    image: openmetadata/server:1.10.9
    container_name: openmetadata-server
    depends_on:
      openmetadata-mysql:
        condition: service_healthy
      openmetadata-elasticsearch:
        condition: service_healthy
      openmetadata-migrations:
        condition: service_completed_successfully
    environment:
      ELASTICSEARCH_HOSTS: http://openmetadata-elasticsearch:9200
      SERVER_PORT: 8585
      DB_HOST: openmetadata-mysql
      DB_PORT: 3306
      DB_SCHEME: mysql
      DB_DRIVER_CLASS: com.mysql.cj.jdbc.Driver
      DB_PARAMS: allowPublicKeyRetrieval=true&useSSL=false&serverTimezone=UTC
      DB_USER: ${OM_DB_USER:-openmetadata}
      DB_USER_PASSWORD: ${OM_DB_PASSWORD:-openmetadata}
      OM_DATABASE: ${OM_DB_NAME:-openmetadata_db}
    volumes:
      - ./openmetadata/config/openmetadata.yaml:/openmetadata/conf/openmetadata.yaml:ro
    ports:
      - "8585:8585"
    restart: unless-stopped


  # -------------------- Superset (profile: omd) --------------------
  superset:
    profiles: ["omd"]
    build:
      context: .
      dockerfile: docker/Dockerfile.superset
    image: tourism-superset:latest
    container_name: superset
    environment:
      SUPERSET_SECRET_KEY: ${SUPERSET_SECRET_KEY:-supersetadmin}
      SUPERSET_ENV: production
      FLASK_ENV: production
      SUPERSET_LOAD_EXAMPLES: "no"
      SUPERSET_PORT: 8088
      SUPERSET_ADMIN_USERNAME: ${SUPERSET_ADMIN_USERNAME:-admin}
      SUPERSET_ADMIN_PASSWORD: ${SUPERSET_ADMIN_PASSWORD:-admin}
      SUPERSET_ADMIN_EMAIL: ${SUPERSET_ADMIN_EMAIL:-admin@superset.local}
    ports:
      - "8088:8088"
    volumes:
      - superset_home:/app/superset_home
    depends_on:
      - clickhouse
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://localhost:8088/health || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 20
      start_period: 30s
    restart: unless-stopped

  superset-init:
    profiles: ["omd"]
    image: tourism-superset:latest
    container_name: superset-init
    depends_on:
      superset:
        condition: service_started
    entrypoint: /bin/bash
    command: >
      -lc "
      superset db upgrade &&
      superset fab create-admin
        --username ${SUPERSET_ADMIN_USERNAME:-admin}
        --firstname Admin --lastname User
        --email ${SUPERSET_ADMIN_EMAIL:-admin@superset.local}
        --password ${SUPERSET_ADMIN_PASSWORD:-admin} || true &&
      superset init"
    restart: "no"

  # -------------------- One-shot OM ingestion helpers (profile: omd) --------------------
  om-ingest-clickhouse:
    profiles: ["omd"]
    image: openmetadata/ingestion:1.10.9
    container_name: om-ingest-clickhouse
    entrypoint: [ "bash", "-lc" ]
    command: "metadata ingest -c /ingestion/clickhouse.yaml"
    volumes:
      - ./openmetadata/ingestion:/ingestion
    depends_on:
      openmetadata-server:
        condition: service_healthy
    restart: "no"

  om-ingest-superset:
    profiles: ["omd"]
    image: openmetadata/ingestion:1.10.9
    container_name: om-ingest-superset
    entrypoint: [ "bash", "-lc" ]
    command: "metadata ingest -c /ingestion/superset.yaml"
    volumes:
      - ./openmetadata/ingestion:/ingestion
    depends_on:
      openmetadata-server:
        condition: service_healthy
      superset:
        condition: service_healthy
    restart: "no"

volumes:
  clickhouse_data: {}
  iceberg_warehouse: {}
  minio_data: {}
  airflow_db_data: {}
  om_mysql_data: {}
  om_es_data: {}
  superset_home: {}
